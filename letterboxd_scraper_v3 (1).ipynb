{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb09e9e2-22e3-4f3c-b4a2-476b77380c17",
   "metadata": {},
   "source": [
    "# Collecting the first 500 pages of movie titles ordered by most popular "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20e4d9-3c86-4943-8d26-bf9dbc40f328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import csv\n",
    "import re\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions() \n",
    "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\") # disabling images to improve run time \n",
    "chrome_options.add_argument('--headless')  # Enable headless mode\n",
    "chrome_options.add_argument('--disable-gpu')  # Disable GPU acceleration in headless mode (optional)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "\n",
    "fields = [\"Link\", \"Title\", \"Year\", \"Rating\"] # creating data columns\n",
    "filename = \"Letterboxd_scrape_first500.csv\" # new file\n",
    "# creating lists \n",
    "rows = []\n",
    "links = []\n",
    "titles_temp = [] #temporary container for movie info \n",
    "titles = []\n",
    "year = []\n",
    "rating = []\n",
    "errors = []\n",
    "synopsis = []\n",
    "\n",
    "for i in range(1,501):\n",
    "    url = f\"https://letterboxd.com/films/popular/page/{i}/\"\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        #movies = driver.find_elements(By.XPATH, \"//*[@id='films-browser-list-container']/ul/li\") # container of all movies on page\n",
    "        # i put by css selector because it is supposed to be faster\n",
    "        movies = driver.find_elements(By.CSS_SELECTOR, \"#films-browser-list-container ul li\")\n",
    "        for movie in movies: # iterate through movies \n",
    "            movie_info = movie.find_element(By.TAG_NAME, \"a\").get_attribute('data-original-title')\n",
    "            if movie_info.find(\"(\") == -1 or movie_info[-1].isdigit() == False:\n",
    "                continue\n",
    "            titles_temp.append(movie_info) # movie info\n",
    "            link = movie.find_element(By.TAG_NAME, \"a\").get_attribute('href')\n",
    "            links.append(link) # link\n",
    "\n",
    "    except: #print page number of where error occurs\n",
    "        errors.append(i)\n",
    "        continue\n",
    "for i in errors:\n",
    "    url = f\"https://letterboxd.com/films/popular/page/{i}/\"\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        #movies = driver.find_elements(By.XPATH, \"//*[@id='films-browser-list-container']/ul/li\") # container of all movies on page\n",
    "        # i put by css selector because it is supposed to be faster\n",
    "        movies = driver.find_elements(By.CSS_SELECTOR, \"#films-browser-list-container ul li\")\n",
    "        for movie in movies: # iterate through movies \n",
    "\n",
    "            movie_info = movie.find_element(By.TAG_NAME, \"a\").get_attribute('data-original-title')\n",
    "            if movie_info.find(\"(\") == -1 or movie_info[-1].isdigit() == False:\n",
    "                continue\n",
    "            titles_temp.append(movie_info) # movie info\n",
    "            link = movie.find_element(By.TAG_NAME, \"a\").get_attribute('href')\n",
    "            links.append(link) # link\n",
    "\n",
    "    except: #print page number of where error occurs\n",
    "        print(\"page: \"+str(i))\n",
    "        continue\n",
    "\n",
    "for film in titles_temp: #iterate through movie info for each one\n",
    "    try:\n",
    "    # append separate features of the movie info \n",
    "        titles.append(film[:-11].strip()) \n",
    "        year.append(film[-10:-6].strip())\n",
    "        rating.append(film[-4:].strip())\n",
    "    # print movie info when there's an error to track it \n",
    "    except IndexError:\n",
    "        print(new_list)\n",
    "        print(film)\n",
    "        continue\n",
    "for i in range(len(titles)-1): \n",
    "    try:\n",
    "        rows.append([links[i], titles[i], year[i], rating[i]]) # create rows of csv \n",
    "    except IndexError:\n",
    "        print(i) # print index where error occurs \n",
    "with open(filename, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields) \n",
    "    csvwriter.writerows(rows)\n",
    "\n",
    "driver.quit()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928c284-3514-46b9-b82d-36f306e16af2",
   "metadata": {},
   "source": [
    "# Collecting synopsis for each movie title\n",
    "### (This data ended up not being utilized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a48917-36cd-4a5e-860d-1f447579efe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "import requests_cache\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import cchardet as chardet\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#for speeding up BeautifulSoup requests by saving cache for 24 hours\n",
    "requests_cache.install_cache('my_cache', expire_after=86400)\n",
    "requests_session = requests.Session()\n",
    "\n",
    "df = pd.read_csv('Letterboxd_scrape_first500.csv')\n",
    "synopsis = []\n",
    "for link in df[\"Link\"]: #get each link from dataframe\n",
    "    try:\n",
    "        \n",
    "        source = requests_session.get(link)\n",
    "        soup = BeautifulSoup(source.content, 'lxml')\n",
    "        #finds where the synopsis is located and get the text\n",
    "        main = soup.find(\"div\", class_ = \"content-wrap\")\n",
    "        main = main.find(\"div\", class_ = \"review body-text -prose -hero prettify\")\n",
    "        synop = main.find(\"div\", class_ = 'truncate').p.text \n",
    "        #add to synopsis list\n",
    "        synopsis.append(synop)\n",
    "    except:\n",
    "        #if synopsis is unavailable, add nothing to the list\n",
    "        synopsis.append(\"\")\n",
    "        continue\n",
    "#add the synopsis list into a dataframe column\n",
    "df['Synopsis'] = pd.Series(synopsis)\n",
    "df.to_csv('Letterboxd_scrape_first500_v2.csv')  \n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
